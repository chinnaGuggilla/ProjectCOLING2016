Transformers in NLP have several architectures like Encoder only, Decoder only, Encoder-Decoder, etc.

Which architecture is best for a task might be a bit confusing if you are starting out. Here are a few pointers -

➤ Encoder-only models like BERT are designed to produce a sequence of embeddings that we usually pool to get a single prediction/multiple predictions for a given input sequence. This makes them better suited for text classification or span prediction tasks.

➤ The decoder-only models like GPT-2 can take an input sequence and generate the rest of the text based on the input sequence. So it is well suited to fill up an input prompt creatively. So GPT-2 doesn't really transform (rewrite) input but just extends it further.

➤ Transformer models like T5 use the encoder-decoder architecture to perform any sequence to sequence (seq2seq) tasks effectively. (Translation, Summarization, etc). Here we actually transform(rewrite) the input. If we rewrite it concisely it is called summarization. If we rewrite in another language it is called translation.

Although you can use BERT for text generation and GPT-2 for seq2seq using some tricks, their primary strength may not be that.