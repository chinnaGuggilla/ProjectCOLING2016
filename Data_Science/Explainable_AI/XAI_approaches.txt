Evaluation of the quality of explanations using the methods mentioned in the box above requires in-depth knowledge of the mathematical operations such as derivatives, layer-wise feature mapping, perturbations, attention mechanisms, and others. Giplin et al. present a survey of "explaining" explanations and show that human evaluators are needed to evaluate explanations produced by a model [7]. Due to the nature of the explanations, the current evaluation of the explanations is limited to analysis of the word and token level feature importance once a suitable visualization mechanism, such as a saliency map, is utilized. Notably, the mathematical expertise required to "open the black box" has been a critical bottleneck in adopting AI systems with explanations. Domain experts require explanations in a language they can easily comprehend or understand to evaluate the system. For example, in the medical domain, the outcome of a model needs to be explained by positioning against conceptual knowledge contained in clinical guidelines.
